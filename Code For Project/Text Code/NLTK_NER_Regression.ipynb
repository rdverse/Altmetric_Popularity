{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for printing the csv file with title and abstract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "ultimate_dict ={}\n",
    "\n",
    "def getvalues(PATH) :\n",
    "    \n",
    "    filenames = os.listdir(PATH)\n",
    "    ultimate_dict = {}\n",
    "    foldernames = os.listdir(PATH)\n",
    "\n",
    "    myFields = ['Altmetric_ID', 'Altmetric_Score', 'Title', 'Abstract', 'Scopus']\n",
    "    df = pd.DataFrame(columns = ['Altmetric_ID', 'Altmetric_Score', 'Title', 'Abstract', 'Scopus'])\n",
    "\n",
    "    count = 0\n",
    "    for folder in foldernames:\n",
    "        filenames = os.listdir(PATH + folder)\n",
    "        for file in filenames:\n",
    "            \n",
    "            with open(PATH + folder + '\\\\' +file,encoding=\"ISO-8859-1\") as f:\n",
    "                data1 = {}\n",
    "                for line in f:\n",
    "                    data1 = json.loads(line)\n",
    "                    break\n",
    "                    #print(data1)\n",
    "                    \n",
    "                try:\n",
    "\n",
    "                    ultimate_dict['Altmetric_ID']=(data1['altmetric_id'])\n",
    "                except KeyError:\n",
    "                    ultimate_dict['Altmetric_ID']= np.NaN\n",
    "                    break\n",
    "\n",
    "\n",
    "                try:\n",
    "                    ultimate_dict['Altmetric_Score']=(data1[\"altmetric_score\"][\"score\"])\n",
    "                except KeyError:\n",
    "                    ultimate_dict['Altmetric_Score']=np.NaN\n",
    "\n",
    "\n",
    "                try:\n",
    "                    ultimate_dict['Title']=(data1['citation']['title'])\n",
    "                except KeyError:\n",
    "                    ultimate_dict['Title']=np.NaN\n",
    "\n",
    "\n",
    "                try:\n",
    "                    ultimate_dict['Abstract']= (data1['citation']['abstract'])\n",
    "\n",
    "                except KeyError:\n",
    "                    ultimate_dict['Abstract']=np.NaN\n",
    "\n",
    "\n",
    "\n",
    "                try:\n",
    "                    ultimate_dict['Scopus']=\" \".join(data1['citation']['scopus_subjects'])\n",
    "                except KeyError:\n",
    "                    ultimate_dict['Scopus']=np.NaN\n",
    "\n",
    "\n",
    "\n",
    "                df.loc[-1] = ultimate_dict  # adding a row\n",
    "                df.index = df.index + 1  # shifting index\n",
    "                df = df.sort_index()\n",
    "                df = df.dropna(axis=0, how='any')\n",
    "                print(count)\n",
    "                count = count +1\n",
    "                clear_output(wait=True)\n",
    "\n",
    "    clean_df = df.dropna(axis=0,how = 'any')\n",
    "    clean_df[myFields].to_csv('C:\\\\Users\\\\Devesh\\\\Documents\\\\data\\\\extractone\\\\jun-10th-dump.tar.gz.part\\\\keys\\\\key.csv', index = None, header = True)\n",
    "\n",
    "\n",
    "PATH = 'C:\\\\Users\\\\Devesh\\\\Documents\\\\data\\\\extractone\\\\jun-10th-dump.tar.gz.part\\\\keys\\\\' \n",
    "getvalues(PATH) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for comparing title and abstracts \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "import nltk\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from math import sqrt, log\n",
    "from collections import defaultdict\n",
    "from itertools import chain, product\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#also need to download gensim on the command prompt, for (windows,anaconda) it is conda install gensim\n",
    "\n",
    "# function to split the title and abstract into tokens, control moves to stoken_clean\n",
    "def split_tokenize(title_string, abstract_string ):\n",
    "\n",
    "    title_sentences = nltk.sent_tokenize(title_string)\n",
    "    abstract_sentences = nltk.sent_tokenize(abstract_string)\n",
    "    \n",
    "    title_tokens = [word_tokenize(sent) for sent in title_sentences]\n",
    "    abstract_tokens = [word_tokenize(sent) for sent in abstract_sentences]\n",
    "\n",
    "\n",
    "    return(title_tokens,abstract_tokens)\n",
    "    \n",
    "    \n",
    "#this function converts words to lower case, removes stopwords, output goes for bagging\n",
    "def token_clean(title_tokens, abstract_tokens): \n",
    "    \n",
    "    title_tokens_loweralpha = [it.lower() for it in title_tokens\n",
    "                              if(it not in stopwords.words('english'))]\n",
    "    abstract_tokens_loweralpha = [it.lower() for it in abstract_tokens\n",
    "                                  if(it not in stopwords.words('english'))]\n",
    "    \n",
    "#    return title_tokens_loweralpha,abstract_tokens_loweralpha\n",
    "    WDL = WordNetLemmatizer()\n",
    "    \n",
    "    title_clean = [WDL.lemmatize(it) for it in title_tokens_loweralpha]\n",
    "    abstract_clean = [WDL.lemmatize(it) for it in abstract_tokens_loweralpha]\n",
    "   \n",
    "    return(title_clean,abstract_clean)\n",
    "\n",
    "    \n",
    "def NER(title_stem, abstract_stem):\n",
    "    #print(abstract_stem)\n",
    "    title_NER = (nltk.pos_tag(title_stem))\n",
    "    abstract_NER = (nltk.pos_tag(abstract_stem))\n",
    "    title_str = \" \"\n",
    "    abstract_str = \" \"\n",
    "    abstract_strr = \" \" \n",
    "    for key,label in title_NER:\n",
    "        title_str = title_str +\" \" + label\n",
    "    \n",
    "    for key,label in abstract_NER:\n",
    "        abstract_str = abstract_str + \" \" + label\n",
    "        abstract_strr = abstract_strr + \" \" + key    \n",
    "    #print(title_str)\n",
    "    #print(abstract_str)\n",
    "    #print(abstract_strr)\n",
    "    return(title_str,abstract_str)\n",
    "    \n",
    "\n",
    "def cosine_sime(sentence_a, sentence_b):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(tuple((sentence_a,sentence_b)))\n",
    "    return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)[0][1]\n",
    "\n",
    "\n",
    "pat = 'C:\\\\Users\\\\Devesh\\\\Documents\\\\data\\\\extractone\\\\jun-10th-dump.tar.gz.part\\\\keys\\\\key.csv'\n",
    "theexcel = pd.read_csv(pat)\n",
    "ultimate_dict = {'Altmetric_ID': '','Altmetric_Score':'','Cosine': ''}\n",
    "cosine_list = []\n",
    "df = pd.DataFrame(columns = ['Altmetric_ID', 'Altmetric_Score','Cosine'])\n",
    "myFields = ['Altmetric_ID', 'Altmetric_Score', 'Cosine']\n",
    "count = 0\n",
    "for i in range(0,26080):\n",
    "    title_token, abstract_token = split_tokenize(theexcel[['Title']].iloc[[i]].to_string(),theexcel[['Abstract']].iloc[[i]].to_string())\n",
    "    title_stem,abstract_stem = token_clean(title_token[0], abstract_token[0])\n",
    "    titleNER,abstractNER = NER(title_stem[2:],abstract_stem[2:])\n",
    "    \n",
    "    ultimate_dict['Altmetric_ID'] = theexcel['Altmetric_ID'].iloc[i]\n",
    "    ultimate_dict['Altmetric_Score'] = theexcel['Altmetric_Score'].iloc[i]\n",
    "    ultimate_dict['Cosine'] = (cosine_sime(titleNER,abstractNER))\n",
    "    print(theexcel[['Altmetric_ID']].iloc[[i]])\n",
    "    \n",
    "    df.loc[-1] = ultimate_dict  # adding a row\n",
    "    df.index = df.index + 1  # shifting index\n",
    "    df = df.sort_index()\n",
    "    df = df.dropna(axis=0, how='any')\n",
    "    print(count)\n",
    "    count = count +1\n",
    "    #\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "\n",
    "    \n",
    "    ultimate_dict['Altmetric_ID']= np.NaN\n",
    "    ultimate_dict['Altmetric_Score']= np.NaN\n",
    "    ultimate_dict['Cosine']= np.NaN\n",
    "\n",
    "#set the path to title_abstract.csv \n",
    "PATH ='C:\\\\Users\\\\Devesh\\\\Documents\\\\data\\\\extractone\\\\jun-10th-dump.tar.gz.part\\\\keyt\\\\keytonisi.csv'\n",
    "df[myFields].to_csv(PATH, index = None, header = True)\n",
    "x = df['Altmetric_Score']\n",
    "y = df['Cosine']\n",
    "plt.scatter(y,x)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression for cosine and Altmetric Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Altmetric_Score']\n",
    "y = df['Cosine']\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "reg = LinearRegression()\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "X = np.array(df['Altmetric_Score'])\n",
    "y = np.array(df['Cosine'])\n",
    "X = X.reshape(-1,1)\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 21)\n",
    "\n",
    "reg.fit(X_train,y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "\n",
    "reg.fit(X_train,y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "plt.ylabel(\"Cosine Similarities of test sample\")\n",
    "plt.xlabel(\"predicted altmetric score\")\n",
    "plt.scatter(X_test,y_pred, color = 'black',edgecolor='blue')\n",
    "\n",
    "\n",
    "print(df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
